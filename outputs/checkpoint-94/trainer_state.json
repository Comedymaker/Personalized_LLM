{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9973474801061007,
  "eval_steps": 500,
  "global_step": 94,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010610079575596816,
      "grad_norm": 2.310270309448242,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 4.8967,
      "step": 1
    },
    {
      "epoch": 0.021220159151193633,
      "grad_norm": 2.257821559906006,
      "learning_rate": 4.000000000000001e-06,
      "loss": 4.8638,
      "step": 2
    },
    {
      "epoch": 0.03183023872679045,
      "grad_norm": 2.302016019821167,
      "learning_rate": 6e-06,
      "loss": 4.8339,
      "step": 3
    },
    {
      "epoch": 0.042440318302387266,
      "grad_norm": 2.1854441165924072,
      "learning_rate": 8.000000000000001e-06,
      "loss": 4.882,
      "step": 4
    },
    {
      "epoch": 0.05305039787798409,
      "grad_norm": 2.2418012619018555,
      "learning_rate": 1e-05,
      "loss": 4.8903,
      "step": 5
    },
    {
      "epoch": 0.0636604774535809,
      "grad_norm": 2.2013003826141357,
      "learning_rate": 1.2e-05,
      "loss": 4.8991,
      "step": 6
    },
    {
      "epoch": 0.07427055702917772,
      "grad_norm": 2.2659144401550293,
      "learning_rate": 1.4e-05,
      "loss": 4.791,
      "step": 7
    },
    {
      "epoch": 0.08488063660477453,
      "grad_norm": 2.356858253479004,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 4.8486,
      "step": 8
    },
    {
      "epoch": 0.09549071618037135,
      "grad_norm": 2.281806468963623,
      "learning_rate": 1.8e-05,
      "loss": 4.7992,
      "step": 9
    },
    {
      "epoch": 0.10610079575596817,
      "grad_norm": 2.1651251316070557,
      "learning_rate": 2e-05,
      "loss": 4.767,
      "step": 10
    },
    {
      "epoch": 0.11671087533156499,
      "grad_norm": 2.2765543460845947,
      "learning_rate": 1.9761904761904763e-05,
      "loss": 4.7799,
      "step": 11
    },
    {
      "epoch": 0.1273209549071618,
      "grad_norm": 2.200310468673706,
      "learning_rate": 1.9523809523809524e-05,
      "loss": 4.705,
      "step": 12
    },
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 2.059007406234741,
      "learning_rate": 1.928571428571429e-05,
      "loss": 4.6596,
      "step": 13
    },
    {
      "epoch": 0.14854111405835543,
      "grad_norm": 2.0557570457458496,
      "learning_rate": 1.904761904761905e-05,
      "loss": 4.6297,
      "step": 14
    },
    {
      "epoch": 0.15915119363395225,
      "grad_norm": 1.8565906286239624,
      "learning_rate": 1.880952380952381e-05,
      "loss": 4.6128,
      "step": 15
    },
    {
      "epoch": 0.16976127320954906,
      "grad_norm": 1.7304823398590088,
      "learning_rate": 1.8571428571428575e-05,
      "loss": 4.5706,
      "step": 16
    },
    {
      "epoch": 0.18037135278514588,
      "grad_norm": 1.5327904224395752,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 4.5382,
      "step": 17
    },
    {
      "epoch": 0.1909814323607427,
      "grad_norm": 1.480475902557373,
      "learning_rate": 1.8095238095238097e-05,
      "loss": 4.5219,
      "step": 18
    },
    {
      "epoch": 0.20159151193633953,
      "grad_norm": 1.309133768081665,
      "learning_rate": 1.785714285714286e-05,
      "loss": 4.459,
      "step": 19
    },
    {
      "epoch": 0.21220159151193635,
      "grad_norm": 1.1774230003356934,
      "learning_rate": 1.761904761904762e-05,
      "loss": 4.4424,
      "step": 20
    },
    {
      "epoch": 0.22281167108753316,
      "grad_norm": 1.2359248399734497,
      "learning_rate": 1.7380952380952384e-05,
      "loss": 4.4484,
      "step": 21
    },
    {
      "epoch": 0.23342175066312998,
      "grad_norm": 1.073711633682251,
      "learning_rate": 1.7142857142857142e-05,
      "loss": 4.3697,
      "step": 22
    },
    {
      "epoch": 0.2440318302387268,
      "grad_norm": 1.0492337942123413,
      "learning_rate": 1.6904761904761906e-05,
      "loss": 4.381,
      "step": 23
    },
    {
      "epoch": 0.2546419098143236,
      "grad_norm": 1.0871938467025757,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 4.3429,
      "step": 24
    },
    {
      "epoch": 0.26525198938992045,
      "grad_norm": 0.9203954339027405,
      "learning_rate": 1.642857142857143e-05,
      "loss": 4.3309,
      "step": 25
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 0.9132876396179199,
      "learning_rate": 1.6190476190476193e-05,
      "loss": 4.3189,
      "step": 26
    },
    {
      "epoch": 0.2864721485411141,
      "grad_norm": 0.959171712398529,
      "learning_rate": 1.5952380952380954e-05,
      "loss": 4.2767,
      "step": 27
    },
    {
      "epoch": 0.29708222811671087,
      "grad_norm": 0.8654213547706604,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 4.2825,
      "step": 28
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 0.8454081416130066,
      "learning_rate": 1.5476190476190476e-05,
      "loss": 4.2717,
      "step": 29
    },
    {
      "epoch": 0.3183023872679045,
      "grad_norm": 0.8942322134971619,
      "learning_rate": 1.523809523809524e-05,
      "loss": 4.2575,
      "step": 30
    },
    {
      "epoch": 0.32891246684350134,
      "grad_norm": 0.811410665512085,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 4.2439,
      "step": 31
    },
    {
      "epoch": 0.3395225464190981,
      "grad_norm": 0.7689286470413208,
      "learning_rate": 1.4761904761904763e-05,
      "loss": 4.2275,
      "step": 32
    },
    {
      "epoch": 0.35013262599469497,
      "grad_norm": 0.7827208638191223,
      "learning_rate": 1.4523809523809524e-05,
      "loss": 4.2043,
      "step": 33
    },
    {
      "epoch": 0.36074270557029176,
      "grad_norm": 0.7508720755577087,
      "learning_rate": 1.4285714285714287e-05,
      "loss": 4.1946,
      "step": 34
    },
    {
      "epoch": 0.3713527851458886,
      "grad_norm": 0.7675748467445374,
      "learning_rate": 1.4047619047619048e-05,
      "loss": 4.1864,
      "step": 35
    },
    {
      "epoch": 0.3819628647214854,
      "grad_norm": 0.7707952260971069,
      "learning_rate": 1.3809523809523811e-05,
      "loss": 4.1645,
      "step": 36
    },
    {
      "epoch": 0.3925729442970822,
      "grad_norm": 0.7825920581817627,
      "learning_rate": 1.3571428571428574e-05,
      "loss": 4.1642,
      "step": 37
    },
    {
      "epoch": 0.40318302387267907,
      "grad_norm": 0.8003215789794922,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 4.1302,
      "step": 38
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 0.7642289996147156,
      "learning_rate": 1.3095238095238096e-05,
      "loss": 4.1328,
      "step": 39
    },
    {
      "epoch": 0.4244031830238727,
      "grad_norm": 0.7745412588119507,
      "learning_rate": 1.2857142857142859e-05,
      "loss": 4.1293,
      "step": 40
    },
    {
      "epoch": 0.4350132625994695,
      "grad_norm": 0.7470276951789856,
      "learning_rate": 1.261904761904762e-05,
      "loss": 4.1015,
      "step": 41
    },
    {
      "epoch": 0.44562334217506633,
      "grad_norm": 0.793340802192688,
      "learning_rate": 1.2380952380952383e-05,
      "loss": 4.0979,
      "step": 42
    },
    {
      "epoch": 0.4562334217506631,
      "grad_norm": 0.7525438070297241,
      "learning_rate": 1.2142857142857142e-05,
      "loss": 4.1179,
      "step": 43
    },
    {
      "epoch": 0.46684350132625996,
      "grad_norm": 0.7825291156768799,
      "learning_rate": 1.1904761904761905e-05,
      "loss": 4.1076,
      "step": 44
    },
    {
      "epoch": 0.47745358090185674,
      "grad_norm": 0.7988024950027466,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 4.0744,
      "step": 45
    },
    {
      "epoch": 0.4880636604774536,
      "grad_norm": 0.7582821249961853,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 4.0647,
      "step": 46
    },
    {
      "epoch": 0.4986737400530504,
      "grad_norm": 0.8074766993522644,
      "learning_rate": 1.1190476190476192e-05,
      "loss": 4.0615,
      "step": 47
    },
    {
      "epoch": 0.5092838196286472,
      "grad_norm": 0.7270247936248779,
      "learning_rate": 1.0952380952380955e-05,
      "loss": 4.0755,
      "step": 48
    },
    {
      "epoch": 0.519893899204244,
      "grad_norm": 0.7767596244812012,
      "learning_rate": 1.0714285714285714e-05,
      "loss": 4.0713,
      "step": 49
    },
    {
      "epoch": 0.5305039787798409,
      "grad_norm": 0.742392361164093,
      "learning_rate": 1.0476190476190477e-05,
      "loss": 4.025,
      "step": 50
    },
    {
      "epoch": 0.5411140583554377,
      "grad_norm": 0.758925199508667,
      "learning_rate": 1.0238095238095238e-05,
      "loss": 4.0311,
      "step": 51
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 0.8127023577690125,
      "learning_rate": 1e-05,
      "loss": 4.025,
      "step": 52
    },
    {
      "epoch": 0.5623342175066313,
      "grad_norm": 0.7399987578392029,
      "learning_rate": 9.761904761904762e-06,
      "loss": 4.0288,
      "step": 53
    },
    {
      "epoch": 0.5729442970822282,
      "grad_norm": 0.7770053744316101,
      "learning_rate": 9.523809523809525e-06,
      "loss": 4.0109,
      "step": 54
    },
    {
      "epoch": 0.583554376657825,
      "grad_norm": 0.756092369556427,
      "learning_rate": 9.285714285714288e-06,
      "loss": 3.991,
      "step": 55
    },
    {
      "epoch": 0.5941644562334217,
      "grad_norm": 0.7763499021530151,
      "learning_rate": 9.047619047619049e-06,
      "loss": 4.0127,
      "step": 56
    },
    {
      "epoch": 0.6047745358090185,
      "grad_norm": 0.8087713718414307,
      "learning_rate": 8.80952380952381e-06,
      "loss": 4.0023,
      "step": 57
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 0.7578100562095642,
      "learning_rate": 8.571428571428571e-06,
      "loss": 3.9964,
      "step": 58
    },
    {
      "epoch": 0.6259946949602122,
      "grad_norm": 0.7600986957550049,
      "learning_rate": 8.333333333333334e-06,
      "loss": 3.977,
      "step": 59
    },
    {
      "epoch": 0.636604774535809,
      "grad_norm": 0.7667102813720703,
      "learning_rate": 8.095238095238097e-06,
      "loss": 3.9964,
      "step": 60
    },
    {
      "epoch": 0.6472148541114059,
      "grad_norm": 0.7662025690078735,
      "learning_rate": 7.857142857142858e-06,
      "loss": 3.9954,
      "step": 61
    },
    {
      "epoch": 0.6578249336870027,
      "grad_norm": 0.7695186734199524,
      "learning_rate": 7.61904761904762e-06,
      "loss": 3.9666,
      "step": 62
    },
    {
      "epoch": 0.6684350132625995,
      "grad_norm": 0.7401809692382812,
      "learning_rate": 7.380952380952382e-06,
      "loss": 3.9749,
      "step": 63
    },
    {
      "epoch": 0.6790450928381963,
      "grad_norm": 0.751841127872467,
      "learning_rate": 7.1428571428571436e-06,
      "loss": 3.9254,
      "step": 64
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 0.7466264963150024,
      "learning_rate": 6.9047619047619055e-06,
      "loss": 3.9493,
      "step": 65
    },
    {
      "epoch": 0.7002652519893899,
      "grad_norm": 0.7644298076629639,
      "learning_rate": 6.666666666666667e-06,
      "loss": 3.9628,
      "step": 66
    },
    {
      "epoch": 0.7108753315649867,
      "grad_norm": 0.810420036315918,
      "learning_rate": 6.4285714285714295e-06,
      "loss": 3.9381,
      "step": 67
    },
    {
      "epoch": 0.7214854111405835,
      "grad_norm": 0.8251085877418518,
      "learning_rate": 6.1904761904761914e-06,
      "loss": 3.9331,
      "step": 68
    },
    {
      "epoch": 0.7320954907161804,
      "grad_norm": 0.77117919921875,
      "learning_rate": 5.9523809523809525e-06,
      "loss": 3.9269,
      "step": 69
    },
    {
      "epoch": 0.7427055702917772,
      "grad_norm": 0.7831836938858032,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 3.9534,
      "step": 70
    },
    {
      "epoch": 0.753315649867374,
      "grad_norm": 0.7542756199836731,
      "learning_rate": 5.476190476190477e-06,
      "loss": 3.9452,
      "step": 71
    },
    {
      "epoch": 0.7639257294429708,
      "grad_norm": 0.8254461884498596,
      "learning_rate": 5.2380952380952384e-06,
      "loss": 3.9413,
      "step": 72
    },
    {
      "epoch": 0.7745358090185677,
      "grad_norm": 0.8346537947654724,
      "learning_rate": 5e-06,
      "loss": 3.9406,
      "step": 73
    },
    {
      "epoch": 0.7851458885941645,
      "grad_norm": 0.8948437571525574,
      "learning_rate": 4.761904761904762e-06,
      "loss": 3.9208,
      "step": 74
    },
    {
      "epoch": 0.7957559681697612,
      "grad_norm": 0.8242635726928711,
      "learning_rate": 4.523809523809524e-06,
      "loss": 3.9414,
      "step": 75
    },
    {
      "epoch": 0.8063660477453581,
      "grad_norm": 0.80201655626297,
      "learning_rate": 4.2857142857142855e-06,
      "loss": 3.8973,
      "step": 76
    },
    {
      "epoch": 0.8169761273209549,
      "grad_norm": 0.8336827158927917,
      "learning_rate": 4.047619047619048e-06,
      "loss": 3.8862,
      "step": 77
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 0.8546895980834961,
      "learning_rate": 3.80952380952381e-06,
      "loss": 3.8987,
      "step": 78
    },
    {
      "epoch": 0.8381962864721485,
      "grad_norm": 0.8185483813285828,
      "learning_rate": 3.5714285714285718e-06,
      "loss": 3.8895,
      "step": 79
    },
    {
      "epoch": 0.8488063660477454,
      "grad_norm": 0.8314951062202454,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 3.8524,
      "step": 80
    },
    {
      "epoch": 0.8594164456233422,
      "grad_norm": 0.8705918192863464,
      "learning_rate": 3.0952380952380957e-06,
      "loss": 3.8641,
      "step": 81
    },
    {
      "epoch": 0.870026525198939,
      "grad_norm": 0.8476338386535645,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 3.8937,
      "step": 82
    },
    {
      "epoch": 0.8806366047745358,
      "grad_norm": 0.8714267611503601,
      "learning_rate": 2.6190476190476192e-06,
      "loss": 3.8809,
      "step": 83
    },
    {
      "epoch": 0.8912466843501327,
      "grad_norm": 0.8967968821525574,
      "learning_rate": 2.380952380952381e-06,
      "loss": 3.8844,
      "step": 84
    },
    {
      "epoch": 0.9018567639257294,
      "grad_norm": 0.8912258744239807,
      "learning_rate": 2.1428571428571427e-06,
      "loss": 3.8975,
      "step": 85
    },
    {
      "epoch": 0.9124668435013262,
      "grad_norm": 0.8876487612724304,
      "learning_rate": 1.904761904761905e-06,
      "loss": 3.8921,
      "step": 86
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 0.8485371470451355,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 3.8804,
      "step": 87
    },
    {
      "epoch": 0.9336870026525199,
      "grad_norm": 0.8650866150856018,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 3.9176,
      "step": 88
    },
    {
      "epoch": 0.9442970822281167,
      "grad_norm": 0.9165499210357666,
      "learning_rate": 1.1904761904761906e-06,
      "loss": 3.8728,
      "step": 89
    },
    {
      "epoch": 0.9549071618037135,
      "grad_norm": 0.8359355330467224,
      "learning_rate": 9.523809523809525e-07,
      "loss": 3.8875,
      "step": 90
    },
    {
      "epoch": 0.9655172413793104,
      "grad_norm": 2.6129801273345947,
      "learning_rate": 7.142857142857143e-07,
      "loss": 3.8489,
      "step": 91
    },
    {
      "epoch": 0.9761273209549072,
      "grad_norm": 0.8886895775794983,
      "learning_rate": 4.7619047619047623e-07,
      "loss": 3.8747,
      "step": 92
    },
    {
      "epoch": 0.986737400530504,
      "grad_norm": 0.9427151083946228,
      "learning_rate": 2.3809523809523811e-07,
      "loss": 3.8409,
      "step": 93
    },
    {
      "epoch": 0.9973474801061007,
      "grad_norm": 0.8977408409118652,
      "learning_rate": 0.0,
      "loss": 3.876,
      "step": 94
    }
  ],
  "logging_steps": 1,
  "max_steps": 94,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.834114085565235e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
