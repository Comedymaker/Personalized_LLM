100%|██████████| 100/100 [07:34<00:00,  4.54s/it]
{'loss': 2.3039, 'grad_norm': 0.29295551776885986, 'learning_rate': 0.00019510565162951537, 'epoch': 0.07}
{'loss': 2.1564, 'grad_norm': 0.21383914351463318, 'learning_rate': 0.00018090169943749476, 'epoch': 0.14}
{'loss': 2.1556, 'grad_norm': 0.206163689494133, 'learning_rate': 0.00015877852522924732, 'epoch': 0.21}
{'loss': 2.1473, 'grad_norm': 0.21507611870765686, 'learning_rate': 0.00013090169943749476, 'epoch': 0.28}
{'loss': 2.1287, 'grad_norm': 0.20604842901229858, 'learning_rate': 0.0001, 'epoch': 0.35}
{'loss': 2.1377, 'grad_norm': 0.20283864438533783, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.42}
{'loss': 2.1341, 'grad_norm': 0.1998714655637741, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.48}
{'loss': 2.1381, 'grad_norm': 0.20629972219467163, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.55}
{'loss': 2.1203, 'grad_norm': 0.22351253032684326, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.62}
{'loss': 2.1326, 'grad_norm': 0.21245421469211578, 'learning_rate': 0.0, 'epoch': 0.69}
{'train_runtime': 456.0597, 'train_samples_per_second': 14.033, 'train_steps_per_second': 0.219, 'train_loss': 2.155464000701904, 'epoch': 0.69}
100%|██████████| 129/129 [00:29<00:00,  4.43it/s]
Model saved to: results/models/20250309_021204_TinyLlama-1.1B-Chat-v1.0
/opt/miniconda/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
Traceback (most recent call last):
  File "/workspace/Personalized_LLM/run_trainer.py", line 5, in <module>
    finetuner.run()
  File "/workspace/Personalized_LLM/train/tinyModel_trainer.py", line 56, in run
    self._save_model(trainer)
  File "/workspace/Personalized_LLM/train/tinyModel_trainer.py", line 93, in _save_model
    tokenizer.save_pretrained(trainer.args.output_dir + "_merged")
    ^^^^^^^^^
NameError: name 'tokenizer' is not defined. Did you mean: 'self.tokenizer'?
