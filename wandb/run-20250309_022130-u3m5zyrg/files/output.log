100%|██████████| 100/100 [07:33<00:00,  4.54s/it]
{'loss': 2.3038, 'grad_norm': 0.28448620438575745, 'learning_rate': 0.00019510565162951537, 'epoch': 0.07}
{'loss': 2.1568, 'grad_norm': 0.2140369713306427, 'learning_rate': 0.00018090169943749476, 'epoch': 0.14}
{'loss': 2.1555, 'grad_norm': 0.20772159099578857, 'learning_rate': 0.00015877852522924732, 'epoch': 0.21}
{'loss': 2.1474, 'grad_norm': 0.2159004658460617, 'learning_rate': 0.00013090169943749476, 'epoch': 0.28}
{'loss': 2.1289, 'grad_norm': 0.20646168291568756, 'learning_rate': 0.0001, 'epoch': 0.35}
{'loss': 2.138, 'grad_norm': 0.20260652899742126, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.42}
{'loss': 2.1342, 'grad_norm': 0.19961479306221008, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.48}
{'loss': 2.1383, 'grad_norm': 0.20696915686130524, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.55}
{'loss': 2.1204, 'grad_norm': 0.2265288382768631, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.62}
{'loss': 2.1328, 'grad_norm': 0.21174634993076324, 'learning_rate': 0.0, 'epoch': 0.69}
{'train_runtime': 455.6157, 'train_samples_per_second': 14.047, 'train_steps_per_second': 0.219, 'train_loss': 2.15561861038208, 'epoch': 0.69}
100%|██████████| 129/129 [00:29<00:00,  4.44it/s]
Model saved to: results/models/20250309_022127_TinyLlama-1.1B-Chat-v1.0
/opt/miniconda/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
完整模型已保存到：results/models/20250309_022127_TinyLlama-1.1B-Chat-v1.0_merged
